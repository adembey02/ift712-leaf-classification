{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c94da229-2166-44b8-bf18-bb8fe7165c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db32e5c0-6e54-4857-9e4b-203bd1f7fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassifier(ABC):\n",
    "    \"\"\"\n",
    "    Classe abstraite de base pour tous les classifieurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Initialise un classifieur avec un nom.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Nom du classifieur\n",
    "        \"\"\"\n",
    "        ##super().__init__(name)\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Méthode abstraite pour construire le modèle spécifique.\n",
    "        Doit être implémentée par les sous-classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle sur les données fournies.\n",
    "        \n",
    "        Args:\n",
    "            X_train (array-like): Caractéristiques d'entraînement\n",
    "            y_train (array-like): Étiquettes d'entraînement\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.is_fitted = True\n",
    "        print(f\"Le modèle {self.name} a été entraîné avec succès.\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Prédit les étiquettes pour de nouvelles données.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Caractéristiques à prédire\n",
    "        \n",
    "        Returns:\n",
    "            array: Prédictions d'étiquettes\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Le modèle doit être entraîné avant de faire des prédictions.\")\n",
    "        \n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Évalue le modèle sur les données de test.\n",
    "        \n",
    "        Args:\n",
    "            X_test (array-like): Caractéristiques de test\n",
    "            y_test (array-like): Étiquettes de test\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionnaire contenant différentes métriques d'évaluation\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Le modèle doit être entraîné avant l'évaluation.\")\n",
    "        \n",
    "        y_pred = self.predict(X_test)\n",
    "        \n",
    "        # Calcul des métriques\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"\\nÉvaluation du modèle {self.name}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Création du rapport de classification\n",
    "        print(\"\\nRapport de classification détaillé:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "    \n",
    "    def cross_validate(self, X, y, cv=5):\n",
    "        \"\"\"\n",
    "        Effectue une validation croisée sur le modèle.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Caractéristiques\n",
    "            y (array-like): Étiquettes\n",
    "            cv (int): Nombre de plis pour la validation croisée\n",
    "        \n",
    "        Returns:\n",
    "            dict: Résultats de la validation croisée\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Réalisation de la validation croisée\n",
    "        accuracy_scores = cross_val_score(self.model, X, y, cv=cv, scoring='accuracy')\n",
    "        precision_scores = cross_val_score(self.model, X, y, cv=cv, scoring='precision_weighted')\n",
    "        recall_scores = cross_val_score(self.model, X, y, cv=cv, scoring='recall_weighted')\n",
    "        f1_scores = cross_val_score(self.model, X, y, cv=cv, scoring='f1_weighted')\n",
    "        \n",
    "        # Calcul des moyennes et écarts-types\n",
    "        mean_accuracy = np.mean(accuracy_scores)\n",
    "        std_accuracy = np.std(accuracy_scores)\n",
    "        mean_precision = np.mean(precision_scores)\n",
    "        std_precision = np.std(precision_scores)\n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        std_recall = np.std(recall_scores)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        std_f1 = np.std(f1_scores)\n",
    "        \n",
    "        print(f\"\\nRésultats de la validation croisée {cv}-fold pour {self.name}:\")\n",
    "        print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "        print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "        print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "        print(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'accuracy': mean_accuracy,\n",
    "            'accuracy_std': std_accuracy,\n",
    "            'precision': mean_precision,\n",
    "            'precision_std': std_precision,\n",
    "            'recall': mean_recall,\n",
    "            'recall_std': std_recall,\n",
    "            'f1': mean_f1,\n",
    "            'f1_std': std_f1\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_param_grid(self):\n",
    "        \"\"\"\n",
    "        Méthode abstraite pour définir la grille d'hyperparamètres à optimiser.\n",
    "        Doit être implémentée par les sous-classes.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionnaire des hyperparamètres pour GridSearchCV\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def optimize_hyperparameters(self, X, y, cv=5):\n",
    "        \"\"\"\n",
    "        Optimise les hyperparamètres du modèle en utilisant GridSearchCV.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Caractéristiques\n",
    "            y (array-like): Étiquettes\n",
    "            cv (int): Nombre de plis pour la validation croisée\n",
    "        \n",
    "        Returns:\n",
    "            dict: Meilleurs hyperparamètres trouvés\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        param_grid = self.get_param_grid()\n",
    "        \n",
    "        print(f\"\\nRecherche des meilleurs hyperparamètres pour {self.name}...\")\n",
    "        grid_search = GridSearchCV(self.model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        self.best_params = grid_search.best_params_\n",
    "        print(f\"Meilleurs hyperparamètres trouvés: {self.best_params}\")\n",
    "        print(f\"Meilleur score de validation: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Mise à jour du modèle avec les meilleurs paramètres\n",
    "        self.model = grid_search.best_estimator_\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self.best_params\n",
    "    \n",
    "    def plot_confusion_matrix(self, X_test, y_test, figsize=(10, 8), save_path=None):\n",
    "        \"\"\"\n",
    "        Affiche la matrice de confusion pour le modèle.\n",
    "        \n",
    "        Args:\n",
    "            X_test (array-like): Caractéristiques de test\n",
    "            y_test (array-like): Étiquettes de test\n",
    "            figsize (tuple): Taille de la figure\n",
    "            save_path (str, optional): Chemin pour sauvegarder la figure\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Le modèle doit être entraîné avant l'évaluation.\")\n",
    "        \n",
    "        y_pred = self.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=np.unique(y_test), \n",
    "                   yticklabels=np.unique(y_test))\n",
    "        plt.title(f\"Matrice de confusion - {self.name}\")\n",
    "        plt.ylabel('Valeur réelle')\n",
    "        plt.xlabel('Valeur prédite')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Matrice de confusion enregistrée dans {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_roc_curve(self, X_test, y_test, figsize=(10, 8), save_path=None):\n",
    "        \"\"\"\n",
    "        Affiche la courbe ROC pour le modèle (uniquement pour les problèmes multi-classes).\n",
    "        \n",
    "        Args:\n",
    "            X_test (array-like): Caractéristiques de test\n",
    "            y_test (array-like): Étiquettes de test\n",
    "            figsize (tuple): Taille de la figure\n",
    "            save_path (str, optional): Chemin pour sauvegarder la figure\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Le modèle doit être entraîné avant l'évaluation.\")\n",
    "        \n",
    "        # Convertir y_test en format one-hot encoding\n",
    "        classes = np.unique(y_test)\n",
    "        n_classes = len(classes)\n",
    "        \n",
    "        # Vérifier si le modèle peut retourner des probabilités\n",
    "        if not hasattr(self.model, \"predict_proba\"):\n",
    "            print(f\"Le modèle {self.name} ne prend pas en charge le calcul des probabilités, impossible de tracer la courbe ROC.\")\n",
    "            return\n",
    "        \n",
    "        # Binariser les étiquettes\n",
    "        y_test_bin = label_binarize(y_test, classes=classes)\n",
    "        \n",
    "        # Calculer les scores ROC\n",
    "        y_score = self.model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculer la courbe ROC et l'AUC pour chaque classe\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Tracer la courbe ROC pour chaque classe\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        colors = cycle(['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink'])\n",
    "        for i, color, cls in zip(range(n_classes), colors, classes):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                     label=f'Classe {cls} (AUC = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Taux de faux positifs')\n",
    "        plt.ylabel('Taux de vrais positifs')\n",
    "        plt.title(f'Courbe ROC multi-classe - {self.name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Courbe ROC enregistrée dans {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfbf25-69e6-4814-9669-8abf5e9e1164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b4be3-940d-48f4-9aa6-3e2987300f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "”IFT599/IFT799”",
   "language": "python",
   "name": "udes_sd_a25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
